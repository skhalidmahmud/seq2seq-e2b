# seq2seq-e2b
An attention-based English to Bangla machine translation model using sequence to sequence learning with neural networks

A new machine translation approach called Neural Machine Translation (NMT) has shown promising fallouts using order to order (seq2seq) framework that is comparable to traditional approaches. Using this technology, we propose a novel neural network framework for English to Bangla machine translation. In this research, we used attention based RNN Encoderâ€“Decoder which consists of two recurrent neural networks (RNN). One RNN encodes an order of symbols into a fixed length vector representation, and the other decodes the representation into another order of symbols. The encoder and decoder of the proposed framework are jointly trained to maximize the conditional probability of a target order given a source order. A noteworthy weakness in conventional NMT systems is their inability to correctly translate for relatively small vocabularies. We solved that very rare words problem using a <unk> token that represents every possible out-of-vocabulary word using an English to Bangla dictionary. Applying our proposed framework on an English to Bangla dataset, we got a substantial improvement of up to 0.86 BLEU scores and an average of 0.79 BLEU scores.
